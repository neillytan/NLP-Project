{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "\n",
    "#let's use cpu for now\n",
    "context = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "seq = mx.nd.array(np.random.randint(vocab_size, size=100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[15. 22.  6. ... 25. 18.  1.]\n",
       "<NDArray 100000 @cpu(0)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model, we can modify this for more complex model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stolen from https://gluon.mxnet.io/chapter05_recurrent-neural-networks/rnns-gluon.html\n",
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, tie_weights=False, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer = mx.init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu', dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden,\n",
    "                                        params = self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model constants\n",
    "num_embed = 5\n",
    "num_hidden = 5\n",
    "num_layers = 1\n",
    "\n",
    "#training constants\n",
    "args_lr = 0.01\n",
    "args_epochs = 50\n",
    "args_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model and loss\n",
    "model = RNNModel(mode='lstm', vocab_size=vocab_size, num_embed=num_embed, num_hidden=num_hidden,\n",
    "                 num_layers=num_layers)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': args_lr, 'momentum': 0, 'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx=context)\n",
    "    for i in range(0, data_source.shape[0] - 1, args_bptt):\n",
    "        data, target = get_batch(data_source, i)\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal\n",
    "\n",
    "#args_bptt?\n",
    "def train(train_data, args_epochs, args_batch_size, context,\n",
    "         args_bptt=5, args_log_interval=10):\n",
    "    best_val = float(\"Inf\")\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx = context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            #hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and bptt size to balance it. (why?)\n",
    "            #gluon.utils.clip_global_norm(grads, args_clip * args_bptt * args_batch_size)\n",
    "\n",
    "            trainer.step(args_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_batch_size / args_log_interval\n",
    "                #cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        #val_L = eval(val_data)\n",
    "\n",
    "        #print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
    "        #    epoch + 1, time.time() - start_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        #if val_L < best_val:\n",
    "        #    best_val = val_L\n",
    "        #    test_L = eval(test_data)\n",
    "        #    model.save_parameters(args_save)\n",
    "        #    print('test loss %.2f, test perplexity %.2f' % (test_L, math.exp(test_L)))\n",
    "        #else:\n",
    "        #    args_lr = args_lr * 0.25\n",
    "        #    trainer._init_optimizer('sgd',\n",
    "        #                            {'learning_rate': args_lr,\n",
    "        #                             'momentum': 0,\n",
    "        #                             'wd': 0})\n",
    "        #    model.load_parameters(args_save, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "train_data = batchify(seq, args_batch_size).as_in_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_bptt=5\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 10] loss 21.52, perplexity 2209321622.81\n",
      "[Epoch 1 Batch 20] loss 19.56, perplexity 312898480.01\n",
      "[Epoch 1 Batch 30] loss 19.56, perplexity 312721994.45\n",
      "[Epoch 1 Batch 40] loss 19.56, perplexity 312514670.63\n",
      "[Epoch 1 Batch 50] loss 19.56, perplexity 312376054.47\n",
      "[Epoch 1 Batch 60] loss 19.56, perplexity 312548827.56\n",
      "[Epoch 1 Batch 70] loss 19.56, perplexity 312316717.42\n",
      "[Epoch 1 Batch 80] loss 19.56, perplexity 312806346.66\n",
      "[Epoch 1 Batch 90] loss 19.56, perplexity 312933156.39\n",
      "[Epoch 1 Batch 100] loss 19.56, perplexity 312301408.39\n",
      "[Epoch 1 Batch 110] loss 19.56, perplexity 313022103.05\n",
      "[Epoch 1 Batch 120] loss 19.56, perplexity 312535355.09\n",
      "[Epoch 1 Batch 130] loss 19.56, perplexity 313183047.31\n",
      "[Epoch 1 Batch 140] loss 19.56, perplexity 312669390.23\n",
      "[Epoch 1 Batch 150] loss 19.56, perplexity 312441302.48\n",
      "[Epoch 1 Batch 160] loss 19.56, perplexity 312673385.93\n",
      "[Epoch 1 Batch 170] loss 19.56, perplexity 313214648.68\n",
      "[Epoch 1 Batch 180] loss 19.56, perplexity 312904030.36\n",
      "[Epoch 1 Batch 190] loss 19.56, perplexity 312770252.58\n",
      "[Epoch 1 Batch 200] loss 19.56, perplexity 312015440.22\n",
      "[Epoch 1 Batch 210] loss 19.56, perplexity 312570945.12\n",
      "[Epoch 1 Batch 220] loss 19.56, perplexity 312665036.76\n",
      "[Epoch 1 Batch 230] loss 19.56, perplexity 313095607.59\n",
      "[Epoch 1 Batch 240] loss 19.56, perplexity 312771207.08\n",
      "[Epoch 1 Batch 250] loss 19.56, perplexity 313138547.95\n",
      "[Epoch 1 Batch 260] loss 19.56, perplexity 312274128.00\n",
      "[Epoch 1 Batch 270] loss 19.56, perplexity 312486298.77\n",
      "[Epoch 1 Batch 280] loss 19.56, perplexity 313482701.49\n",
      "[Epoch 1 Batch 290] loss 19.56, perplexity 312202185.92\n",
      "[Epoch 1 Batch 300] loss 19.56, perplexity 313108506.99\n",
      "[Epoch 1 Batch 310] loss 19.56, perplexity 312709945.99\n",
      "[Epoch 1 Batch 320] loss 19.56, perplexity 311705713.90\n",
      "[Epoch 1 Batch 330] loss 19.56, perplexity 312153003.28\n",
      "[Epoch 1 Batch 340] loss 19.56, perplexity 312238154.89\n",
      "[Epoch 1 Batch 350] loss 19.56, perplexity 312412162.64\n",
      "[Epoch 1 Batch 360] loss 19.56, perplexity 311811796.36\n",
      "[Epoch 1 Batch 370] loss 19.56, perplexity 311981817.62\n",
      "[Epoch 1 Batch 380] loss 19.56, perplexity 311120056.76\n",
      "[Epoch 1 Batch 390] loss 19.56, perplexity 313412035.22\n",
      "[Epoch 1 Batch 400] loss 19.56, perplexity 312273770.63\n",
      "[Epoch 1 Batch 410] loss 19.56, perplexity 313451969.87\n",
      "[Epoch 1 Batch 420] loss 19.56, perplexity 313023237.44\n",
      "[Epoch 1 Batch 430] loss 19.56, perplexity 312153062.82\n",
      "[Epoch 1 Batch 440] loss 19.56, perplexity 312817145.86\n",
      "[Epoch 1 Batch 450] loss 19.56, perplexity 312284730.14\n",
      "[Epoch 1 Batch 460] loss 19.56, perplexity 313231197.36\n",
      "[Epoch 1 Batch 470] loss 19.56, perplexity 312685015.50\n",
      "[Epoch 1 Batch 480] loss 19.56, perplexity 312242740.63\n",
      "[Epoch 1 Batch 490] loss 19.56, perplexity 312727839.91\n",
      "[Epoch 1 Batch 500] loss 19.56, perplexity 312501914.89\n",
      "[Epoch 1 Batch 510] loss 19.56, perplexity 312609758.96\n",
      "[Epoch 1 Batch 520] loss 19.56, perplexity 312522598.52\n",
      "[Epoch 1 Batch 530] loss 19.56, perplexity 312779141.48\n",
      "[Epoch 1 Batch 540] loss 19.56, perplexity 312324938.15\n",
      "[Epoch 1 Batch 550] loss 19.56, perplexity 312050673.44\n",
      "[Epoch 1 Batch 560] loss 19.56, perplexity 312560512.11\n",
      "[Epoch 1 Batch 570] loss 19.56, perplexity 312612978.76\n",
      "[Epoch 1 Batch 580] loss 19.56, perplexity 312601232.62\n",
      "[Epoch 1 Batch 590] loss 19.56, perplexity 313240696.82\n",
      "[Epoch 1 Batch 600] loss 19.56, perplexity 312883381.17\n",
      "[Epoch 1 Batch 610] loss 19.56, perplexity 312832659.18\n",
      "[Epoch 1 Batch 620] loss 19.56, perplexity 311835824.54\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d512d7a860bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m train(train_data=train_data, args_epochs=args_epochs, args_batch_size=args_batch_size,\n\u001b[0;32m----> 7\u001b[0;31m       context=context, args_bptt=5, args_log_interval=10)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-81ce0063d5da>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, args_epochs, args_batch_size, context, args_bptt, args_log_interval)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mtotal_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mval_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "train(train_data=train_data, args_epochs=args_epochs, args_batch_size=args_batch_size,\n",
    "      context=context, args_bptt=5, args_log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
